# -*- coding: utf-8 -*-
"""Creatanine.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14zcLxpylU3AlxnNTPRZnr7LbEVrKOKRb
"""

# Step 1: Import Libraries
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_squared_error, r2_score
from xgboost import XGBRegressor
import numpy as np

# Step 2: Load the CSV file
# Replace 'data.csv' with the path if it's different
data = pd.read_csv('data.csv')

# Step 3: Preprocess the data
X = data[['Voltage', 'Current']]  # Features
y = data['Concentration']  # Target variable

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Define the models
models = {
    "Gradient Boosting": GradientBoostingRegressor(random_state=42),
    "Random Forest": RandomForestRegressor(random_state=42),
    "KNN": KNeighborsRegressor(),
    "XGBoost": XGBRegressor(random_state=42),
}

# Step 5: Train, predict, and evaluate each model
performance = {}

for model_name, model in models.items():
    print(f"Training {model_name}...")
    model.fit(X_train, y_train)

    # Predict on test data
    y_pred = model.predict(X_test)

    # Evaluate the model
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    performance[model_name] = {'MSE': mse, 'R2': r2}

    print(f"{model_name} - Mean Squared Error: {mse}")
    print(f"{model_name} - R^2 Score: {r2}")
    print("-" * 50)

# Step 6: Tabular Comparison of Metrics
performance_df = pd.DataFrame(performance).T
performance_df.reset_index(inplace=True)
performance_df.columns = ['Model', 'Mean Squared Error (MSE)', 'R² Score']

print("\nPerformance Comparison Table:")
print(performance_df)

# Save performance table to a CSV
performance_df.to_csv("model_performance_comparison.csv", index=False)

# Step 7: Performance Comparison - MSE and R² Bar Graphs
# MSE Bar Graph
plt.figure(figsize=(10, 6))
mse_plot = performance_df.set_index('Model')['Mean Squared Error (MSE)'].plot(kind='bar', color='skyblue')
plt.title('Mean Squared Error for Models')
plt.ylabel('MSE')
plt.xticks(rotation=45)
plt.tight_layout()

# Show MSE values on top of bars
for i, v in enumerate(performance_df['Mean Squared Error (MSE)']):
    mse_plot.text(i, v + 0.02, f"{v:.2f}", ha='center', va='bottom')

plt.show()

# R² Bar Graph
plt.figure(figsize=(10, 6))
r2_plot = performance_df.set_index('Model')['R² Score'].plot(kind='bar', color='lightgreen')
plt.title('R² Score for Models')
plt.ylabel('R² Score')
plt.xticks(rotation=45)
plt.tight_layout()

# Show R² values on top of bars
for i, v in enumerate(performance_df['R² Score']):
    r2_plot.text(i, v + 0.02, f"{v:.2f}", ha='center', va='bottom')

plt.show()

# Step 8: Scatter Plots for Predicted vs Actual values for each model
for model_name, model in models.items():
    print(f"Scatter Plot for {model_name} - Predicted vs Actual")

    # Predict on test data
    y_pred = model.predict(X_test)

    # Scatter plot for Predicted vs Actual
    plt.figure(figsize=(8, 6))
    plt.scatter(y_test, y_pred, alpha=0.7, color='orange')
    plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='blue', lw=2)  # Identity line
    plt.title(f'{model_name} - Predicted vs Actual')
    plt.xlabel('Actual Concentration')
    plt.ylabel('Predicted Concentration')
    plt.tight_layout()
    plt.show()

# Step 9: Correlation Heatmap of Features
plt.figure(figsize=(8, 6))
sns.heatmap(data.corr(), annot=True, fmt='.2f', cmap='coolwarm')
plt.title("Correlation Heatmap of Features")
plt.tight_layout()
plt.show()

# Step 10: True vs Predicted Table for Each Model
for model_name, model in models.items():
    print(f"True vs Predicted Table for {model_name}")

    y_pred = model.predict(X_test)
    true_vs_predicted = pd.DataFrame({
        'Actual': y_test,
        'Predicted': y_pred
    }).reset_index(drop=True)

    print(true_vs_predicted.head(10))  # Display first 10 rows
    true_vs_predicted.to_csv(f"{model_name}_true_vs_predicted.csv", index=False)

# Step 11: Residual Plot for Each Model
for model_name, model in models.items():
    print(f"Residual Plot for {model_name}")

    y_pred = model.predict(X_test)
    residuals = y_test - y_pred

    plt.figure(figsize=(8, 6))
    plt.scatter(y_pred, residuals, alpha=0.7, color='purple')
    plt.axhline(y=0, color='red', linestyle='--')
    plt.title(f'Residual Plot for {model_name}')
    plt.xlabel('Predicted Values')
    plt.ylabel('Residuals')
    plt.tight_layout()
    plt.show()

import pandas as pd
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
from google.colab import files

# Step 1: Load the CSV file (assumed to be uploaded)
data = pd.read_csv('data.csv')  # Replace 'data.csv' with your actual file path or filename

# Step 2: Preprocess the data
# Assuming columns 'Voltage', 'Current', and 'Concentration' are present
X = data[['Voltage', 'Current']]  # Features (adjust according to your dataset)
y = data['Concentration']        # Target variable

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 3: Initialize LightGBM Dataset
train_data = lgb.Dataset(X_train, label=y_train)
test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)

# Step 4: Set parameters for the model
params = {
    'objective': 'regression',       # For regression tasks
    'metric': 'mse',                 # Metric to evaluate
    'boosting_type': 'gbdt',         # Type of boosting (Gradient Boosting Decision Trees)
    'num_leaves': 75,                # Number of leaves in one tree
    'learning_rate': 0.04,           # Learning rate
    'feature_fraction': 0.9          # Fraction of features to use for each tree
}

# Step 5: Train the LightGBM model
gbm = lgb.train(params, train_data, valid_sets=[test_data], num_boost_round=100)

# Step 6: Make predictions
y_pred = gbm.predict(X_test, num_iteration=gbm.best_iteration)

# Step 7: Evaluate the model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f'Mean Squared Error (MSE): {mse}')
print(f'R^2 Score: {r2}')

# Step 8: Make predictions on the uploaded new file
print("Please upload a CSV file for prediction (containing 'Voltage' and 'Current' columns):")
uploaded = files.upload()

# Assume the uploaded file has 'Voltage' and 'Current' columns
for filename in uploaded.keys():
    # Load the uploaded data
    new_scan_data = pd.read_csv(filename)

    # Check if required columns are present
    if not {'Voltage', 'Current'}.issubset(new_scan_data.columns):
        print("The uploaded file must contain 'Voltage' and 'Current' columns.")
    else:
        # Average the 'Voltage' and 'Current' columns to get a single representation for prediction
        voltage_avg = new_scan_data['Voltage'].mean()
        current_avg = new_scan_data['Current'].mean()

        # Create a DataFrame for the single scan to match model input shape
        scan_features = pd.DataFrame([[voltage_avg, current_avg]], columns=['Voltage', 'Current'])

        # Predict the concentration for the uploaded scan data
        concentration_pred = gbm.predict(scan_features)

        # Display the predicted concentration
        print(f"\nPredicted Concentration for the uploaded scan: {concentration_pred[0]:.4f} nM")

import pandas as pd
import numpy as np
from scipy.integrate import trapz
from scipy.stats import skew

# Function to calculate the necessary features for each concentration
def extract_features(data):
    # Initialize a dictionary to store extracted features
    features = {}

    # Extract Peak Height (max current value)
    features['Peak_Height'] = data['Current'].max()

    # Extract Peak Potential (corresponding voltage to Peak Height)
    peak_idx = data['Current'].idxmax()
    features['Peak_Potential'] = data.loc[peak_idx, 'Voltage']

    # Calculate Area Under Curve using trapezoidal rule
    features['Area_Under_Curve'] = trapz(data['Current'], data['Voltage'])

    # Calculate Mean Current
    features['Mean_Current'] = data['Current'].mean()

    # Calculate Standard Deviation of Current
    features['Std_Current'] = data['Current'].std()

    # Calculate Skewness of Current
    features['Skew_Current'] = skew(data['Current'])

    return features

# Function to process the dataset and extract features for each concentration
def process_data(input_file, concentration_range):
    # Read the CSV file
    df = pd.read_csv(input_file)

    # Clean up column names by removing leading/trailing spaces if any
    df.columns = df.columns.str.strip()

    # Check if the expected columns exist in the data
    if 'Concentration' not in df.columns or 'Voltage' not in df.columns or 'Current' not in df.columns:
        print("Error: One or more required columns ('Concentration', 'Voltage', 'Current') are missing.")
        print("Available columns:", df.columns)
        return

    # Create an empty list to store all rows of the output
    output_data = []

    # Process data for each concentration in the given range
    for concentration in concentration_range:
        concentration_data = df[df['Concentration'] == concentration]

        # Extract features for the current concentration
        features = extract_features(concentration_data)
        features['Concentration'] = concentration

        # Append the features along with concentration to the output data
        output_data.append(features)

    # Convert the output data to a DataFrame
    output_df = pd.DataFrame(output_data)

    # Save the output to a new CSV file
    output_df.to_csv('processed_data.csv', index=False)
    print("Processed data saved to 'processed_data.csv'.")

# Ask for file upload
from google.colab import files

print("Please upload your CSV file for processing.")
uploaded = files.upload()

# After file upload, extract the file name
for filename in uploaded.keys():
    input_file = filename
    print(f"File uploaded: {input_file}")

    # Define the concentration range from 5 to 150 with a step of 5
    concentration_range = list(range(5, 151, 5))  # 5, 10, 15, ..., 150

    # Process the data with the specified concentration range
    process_data(input_file, concentration_range)

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    mean_squared_error,
    r2_score,
    confusion_matrix,
    precision_score,
    recall_score,
    fbeta_score,
)
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.integrate import trapz
from scipy.stats import skew, kurtosis

# Function to calculate the necessary features for each concentration
def extract_features(data):
    features = {}

    # Extract Peak Height (maximum current)
    features['Peak_Height'] = data['Current'].max()

    # Extract Peak Potential (Voltage corresponding to Peak Height)
    peak_idx = data['Current'].idxmax()
    features['Peak_Potential'] = data.loc[peak_idx, 'Voltage']

    # Calculate Area Under Curve using trapezoidal rule
    features['Area_Under_Curve'] = trapz(data['Current'], data['Voltage'])

    # Calculate Mean Current
    features['Mean_Current'] = data['Current'].mean()

    # Calculate Standard Deviation of Current
    features['Std_Current'] = data['Current'].std()

    # Calculate Skewness of Current
    features['Skew_Current'] = skew(data['Current'])

    # Calculate Kurtosis of Current
    features['Kurtosis_Current'] = kurtosis(data['Current'])

    # Calculate Voltage Range (max voltage - min voltage)
    features['Voltage_Range'] = data['Voltage'].max() - data['Voltage'].min()

    # Calculate Voltage Centroid (Weighted average of voltage)
    features['Voltage_Centroid'] = (data['Voltage'] * data['Current']).sum() / data['Current'].sum()

    # Calculate Signal-to-Noise Ratio (SNR) as peak current divided by std of current
    features['SNR'] = features['Peak_Height'] / features['Std_Current']

    # Calculate Full Width at Half Maximum (FWHM)
    half_max = features['Peak_Height'] / 2
    half_max_indices = np.where(data['Current'] >= half_max)[0]
    if len(half_max_indices) >= 2:
        fwhm_voltage_range = data['Voltage'].iloc[half_max_indices[-1]] - data['Voltage'].iloc[half_max_indices[0]]
    else:
        fwhm_voltage_range = 0
    features['FWHM'] = fwhm_voltage_range

    # Calculate Derivatives: dI/dV and d²I/dV² (First and Second Derivatives)
    data['dI/dV'] = np.gradient(data['Current'], data['Voltage'])
    data['d2I/dV2'] = np.gradient(data['dI/dV'], data['Voltage'])

    # Maximum values of derivatives
    features['Max_dI/dV'] = data['dI/dV'].max()
    features['Max_d2I/dV2'] = data['d2I/dV2'].max()

    return features

# Load dataset
data = pd.read_csv("new_data.csv")  # Replace with your file path
X = data.drop(columns=["Concentration"])  # Features
y = data["Concentration"]  # Target variable

# Models to evaluate
models = {
    "Random Forest": RandomForestRegressor(n_estimators=100, random_state=42),
    "Gradient Boosting": GradientBoostingRegressor(n_estimators=100, random_state=42),
    "XGBoost": XGBRegressor(n_estimators=100, random_state=42),
    "KNN": KNeighborsRegressor(n_neighbors=5),  # Adding KNN model
}

# Train-Test Ratios
ratios = [0.5, 0.6, 0.7, 0.8]
metrics_df = pd.DataFrame()

# Iterate over train-test ratios and models
for ratio in ratios:
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1 - ratio, random_state=42)

    for name, model in models.items():
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)

        # Compute Metrics
        mse = mean_squared_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        y_pred_rounded = np.round(y_pred)
        y_test_rounded = np.round(y_test)
        cm = confusion_matrix(y_test_rounded, y_pred_rounded)
        precision = precision_score(y_test_rounded, y_pred_rounded, average='weighted', zero_division=0)
        recall = recall_score(y_test_rounded, y_pred_rounded, average='weighted', zero_division=0)
        f2_score = fbeta_score(y_test_rounded, y_pred_rounded, beta=2, average='weighted', zero_division=0)

        # Specificity Calculation
        tn = cm.sum() - (cm.sum(axis=0) + cm.sum(axis=1) - np.diag(cm)).sum()
        fp = cm.sum(axis=0) - np.diag(cm)
        specificity = (tn / (tn + fp.sum())).mean() if (tn + fp.sum()).mean() > 0 else 0

        # Append results to DataFrame
        new_row = pd.DataFrame([{
            "Model": name,
            "Train-Test Ratio": ratio,
            "MSE": mse,
            "R²": r2,
            "Precision": precision,
            "Recall": recall,
            "Specificity": specificity,
            "F2-Score": f2_score
        }])
        metrics_df = pd.concat([metrics_df, new_row], ignore_index=True)

        # Plot Confusion Matrix
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
        plt.title(f"Confusion Matrix for {name} (Train-Test Ratio: {ratio})")
        plt.xlabel("Predicted")
        plt.ylabel("Actual")
        plt.show()

        # Actual vs Predicted Scatter Plot
        plt.figure(figsize=(8, 6))
        plt.scatter(y_test, y_pred, alpha=0.6, color='blue', label='Predicted vs Actual')
        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', linestyle='--', label='Ideal Fit')
        plt.title(f"Actual vs Predicted Values for {name} (Train-Test Ratio: {ratio})")
        plt.xlabel("Actual Concentration")
        plt.ylabel("Predicted Concentration")
        plt.legend()
        plt.grid(alpha=0.3)
        plt.show()

# Display metrics as a table
metrics_df = metrics_df.round(4)
print(metrics_df)

# Visualize Metrics
plt.figure(figsize=(12, 8))
sns.barplot(data=metrics_df, x="Model", y="R²", hue="Train-Test Ratio", ci=None)
plt.title("R² Scores Across Models and Train-Test Ratios")
plt.ylabel("R² Score")
plt.xlabel("Model")
plt.legend(title="Train-Test Ratio")
plt.show()

import pandas as pd
import numpy as np
from scipy.integrate import trapz
from scipy.stats import skew
from xgboost import XGBRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score, precision_score, recall_score, fbeta_score, confusion_matrix
import matplotlib.pyplot as plt
from google.colab import files

# Function to calculate the necessary features for each concentration
def extract_features(data):
    data.rename(columns={'Voltage (V)': 'Voltage', 'Current (A)': 'Current'}, inplace=True)
    features = {}
    features['Peak_Height'] = data['Current'].max()
    peak_idx = data['Current'].idxmax()
    features['Peak_Potential'] = data.loc[peak_idx, 'Voltage']
    features['Area_Under_Curve'] = trapz(data['Current'], data['Voltage'])
    features['Mean_Current'] = data['Current'].mean()
    features['Std_Current'] = data['Current'].std()
    features['Skew_Current'] = skew(data['Current'])
    return features

# Load the dataset
data = pd.read_csv("processed_data.csv")
data.rename(columns={'Voltage (V)': 'Voltage', 'Current (A)': 'Current'}, inplace=True)

# Extract features and target variable (Concentration)
X = data.drop(columns=['Concentration'])  # Features
y = data['Concentration']  # Target variable (Concentration)

# Split the data into train and test sets with 80-20 ratio
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train an XGBoost Regressor model
model = XGBRegressor(n_estimators=100, random_state=42, objective='reg:squarederror')
model.fit(X_train, y_train)

# Predict on training and test data
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)

# Calculate evaluation metrics for training data
mse_train = mean_squared_error(y_train, y_train_pred)
r2_train = r2_score(y_train, y_train_pred)

# Calculate evaluation metrics for test data
mse_test = mean_squared_error(y_test, y_test_pred)
r2_test = r2_score(y_test, y_test_pred)

# Print evaluation details
print(f"Model Evaluation on Training Data:")
print(f"Mean Squared Error (MSE): {mse_train}")
print(f"R-squared (R²): {r2_train}")

print(f"\nModel Evaluation on Test Data:")
print(f"Mean Squared Error (MSE): {mse_test}")
print(f"R-squared (R²): {r2_test}")

# Convert continuous predictions into discrete classes (for classification metrics)
y_train_pred_class = np.round(y_train_pred).astype(int)  # Round predictions to nearest integer
y_test_pred_class = np.round(y_test_pred).astype(int)  # Round predictions to nearest integer
y_train_class = np.round(y_train).astype(int)  # Round actual concentrations to nearest integer
y_test_class = np.round(y_test).astype(int)  # Round actual concentrations to nearest integer

# Calculate confusion matrix and other metrics for test data
cm = confusion_matrix(y_test_class, y_test_pred_class)
precision = precision_score(y_test_class, y_test_pred_class, average='weighted', zero_division=0)
recall = recall_score(y_test_class, y_test_pred_class, average='weighted', zero_division=0)
f2 = fbeta_score(y_test_class, y_test_pred_class, beta=2, average='weighted', zero_division=0)

# Specificity calculation
tn = np.sum(np.diag(cm))
fp = np.sum(cm.sum(axis=0)) - np.sum(np.diag(cm))
specificity = tn / (tn + fp) if tn + fp > 0 else 0

# Print classification metrics
print(f"\nTest Data Evaluation:")
print(f"Precision: {precision:.4f}")
print(f"Recall (Sensitivity): {recall:.4f}")
print(f"F2-Score: {f2:.4f}")
print(f"Specificity: {specificity:.4f}")

# Plot confusion matrix
plt.figure(figsize=(8, 6))
plt.imshow(cm, interpolation='nearest', cmap='Blues')
plt.title("Confusion Matrix (Test Data)")
plt.colorbar()
tick_marks = np.arange(len(np.unique(y_test_class)))
plt.xticks(tick_marks, np.unique(y_test_class))
plt.yticks(tick_marks, np.unique(y_test_class))
plt.xlabel('Predicted')
plt.ylabel('True')
plt.tight_layout()
plt.show()

# Plot Actual vs Predicted values for Test Data
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_test_pred, color='blue', label='Actual vs Predicted')
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], color='red', lw=2, label='Ideal Prediction Line')
plt.xlabel('Actual Concentration')
plt.ylabel('Predicted Concentration')
plt.title('Actual vs Predicted Concentration (Test Data)')
plt.legend()
plt.show()

# Function to process the uploaded data, extract features, and predict concentration
def process_and_predict(input_file):
    df = pd.read_csv(input_file)
    df.columns = df.columns.str.strip()
    print("Columns in the uploaded file:", df.columns)
    print("Sample data:")
    print(df.head())
    expected_columns = {'Voltage (V)': 'Voltage', 'Current (A)': 'Current'}
    df.rename(columns=expected_columns, inplace=True)
    required_columns = ['Voltage', 'Current']
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        print(f"Error: Missing required columns: {missing_columns}")
        return
    features = extract_features(df)
    features_df = pd.DataFrame([features])
    predicted_concentration = model.predict(features_df)
    print(f"Predicted Concentration: {predicted_concentration[0]}")

# Ask for file upload
print("Please upload your CSV file for prediction.")
uploaded = files.upload()

# Process each uploaded file
for filename in uploaded.keys():
    input_file = filename
    print(f"File uploaded: {input_file}")
    process_and_predict(input_file)

import pandas as pd
import numpy as np
from scipy.integrate import trapz
from scipy.stats import skew
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
from google.colab import files

# Function to calculate the necessary features for each concentration
def extract_features(data):
    # Initialize a dictionary to store extracted features
    features = {}

    # Extract Peak Height (max current value)
    features['Peak_Height'] = data['Current'].max()

    # Extract Peak Potential (corresponding voltage to Peak Height)
    peak_idx = data['Current'].idxmax()
    features['Peak_Potential'] = data.loc[peak_idx, 'Voltage']

    # Calculate Area Under Curve using trapezoidal rule
    features['Area_Under_Curve'] = trapz(data['Current'], data['Voltage'])

    # Calculate Mean Current
    features['Mean_Current'] = data['Current'].mean()

    # Calculate Standard Deviation of Current
    features['Std_Current'] = data['Current'].std()

    # Calculate Skewness of Current
    features['Skew_Current'] = skew(data['Current'])

    return features

# Load the training dataset
train_data = pd.read_csv("processed_data.csv")

# Extract features and target variable (Concentration)
X_train = train_data.drop(columns=['Concentration'])  # Features
y_train = train_data['Concentration']  # Target variable (Concentration)

# Train a Random Forest Regressor model
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Predict on training data
y_train_pred = model.predict(X_train)

# Calculate evaluation metrics
mse = mean_squared_error(y_train, y_train_pred)
r2 = r2_score(y_train, y_train_pred)

# Print evaluation details
print(f"Model Evaluation on Training Data:")
print(f"Mean Squared Error (MSE): {mse}")
print(f"R-squared (R²): {r2}")

# Plot Actual vs Predicted values
plt.figure(figsize=(10, 6))
plt.scatter(y_train, y_train_pred, color='blue', label='Actual vs Predicted')
plt.plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], color='red', lw=2, label='Ideal Prediction Line')
plt.xlabel('Actual Concentration')
plt.ylabel('Predicted Concentration')
plt.title('Actual vs Predicted Concentration')
plt.legend()
plt.show()

# Function to process the uploaded data, extract features, and predict concentration
def process_and_predict(input_file):
    # Read the uploaded CSV file
    df = pd.read_csv(input_file)

    # Clean up column names by removing leading/trailing spaces if any
    df.columns = df.columns.str.strip()

    # Check if the expected columns exist in the data
    if 'Voltage' not in df.columns or 'Current' not in df.columns:
        print("Error: One or more required columns ('Voltage', 'Current') are missing.")
        print("Available columns:", df.columns)
        return

    # Extract features for the uploaded scan
    features = extract_features(df)

    # Convert the features into a DataFrame for prediction
    features_df = pd.DataFrame([features])

    # Predict the concentration using the trained model
    predicted_concentration = model.predict(features_df)
    print(f"Predicted Concentration: {predicted_concentration[0]}")

# Ask for file upload
print("Please upload your CSV file for prediction.")
uploaded = files.upload()

# After file upload, extract the file name and process
for filename in uploaded.keys():
    input_file = filename
    print(f"File uploaded: {input_file}")

    # Predict the concentration for the uploaded scan data
    process_and_predict(input_file)

import pandas as pd
import numpy as np
from scipy.integrate import trapz
from scipy.stats import skew, kurtosis

# Function to calculate features from DPV data
def extract_features(data):
    features = {}

    # Extract Peak Height (maximum current)
    features['Peak_Height'] = data['Current'].max()

    # Extract Peak Potential (Voltage corresponding to Peak Height)
    peak_idx = data['Current'].idxmax()
    features['Peak_Potential'] = data.loc[peak_idx, 'Voltage']

    # Calculate Area Under Curve using trapezoidal rule
    features['Area_Under_Curve'] = trapz(data['Current'], data['Voltage'])

    # Calculate Mean Current
    features['Mean_Current'] = data['Current'].mean()

    # Calculate Standard Deviation of Current
    features['Std_Current'] = data['Current'].std()

    # Calculate Skewness of Current
    features['Skew_Current'] = skew(data['Current'])

    # Calculate Kurtosis of Current
    features['Kurtosis_Current'] = kurtosis(data['Current'])

    # Calculate Voltage Range
    features['Voltage_Range'] = data['Voltage'].max() - data['Voltage'].min()

    # Calculate Voltage Centroid
    features['Voltage_Centroid'] = (data['Voltage'] * data['Current']).sum() / data['Current'].sum()

    # Calculate Signal-to-Noise Ratio (SNR)
    features['SNR'] = data['Current'].max() / data['Current'].std()

    # Calculate Full Width at Half Maximum (FWHM)
    half_max = data['Current'].max() / 2
    half_max_indices = np.where(data['Current'] >= half_max)[0]
    if len(half_max_indices) >= 2:
        fwhm_voltage_range = data['Voltage'].iloc[half_max_indices[-1]] - data['Voltage'].iloc[half_max_indices[0]]
    else:
        fwhm_voltage_range = 0
    features['FWHM'] = fwhm_voltage_range

    # Calculate Derivatives
    data['dI/dV'] = np.gradient(data['Current'], data['Voltage'])
    data['d2I/dV2'] = np.gradient(data['dI/dV'], data['Voltage'])
    features['Max_dI/dV'] = data['dI/dV'].max()
    features['Max_d2I/dV2'] = data['d2I/dV2'].max()

    return features

# Main function to process the CSV file and calculate features
def process_file(input_file, output_file="new_data.csv"):
    # Load the input CSV file
    df = pd.read_csv(input_file)

    # Validate required columns
    required_columns = ['Voltage', 'Current', 'Concentration']
    if not all(col in df.columns for col in required_columns):
        raise ValueError(f"The input file must contain the following columns: {required_columns}")

    # Prepare output data
    output_data = []

    # Group data by concentration
    for concentration, group_data in df.groupby('Concentration'):
        # Extract features for each concentration group
        features = extract_features(group_data)
        features['Concentration'] = concentration
        output_data.append(features)

    # Create a DataFrame for the features
    features_df = pd.DataFrame(output_data)

    # Save the features to a new CSV file
    features_df.to_csv(output_file, index=False)
    print(f"Features have been successfully saved to {output_file}")

    # Prompt download if in a notebook environment
    try:
        from google.colab import files
        files.download(output_file)
    except ImportError:
        print(f"The file '{output_file}' is saved locally.")

# Process the file
process_file("data.csv", "new_data.csv")